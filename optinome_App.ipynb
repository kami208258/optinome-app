{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df61c498",
   "metadata": {},
   "source": [
    "# LivenProteins ‚Äî Bioreactor Optimization (Template Rebuild)\n",
    "\n",
    "**Purpose:** Predict **OD600** and **Collagen** from process parameters (DO, Airflow sL/h, Agitation rpm, pH, Feed rate/total, residual glycerol/methanol).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22511331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   LivenProteins ‚Äî Full Helper Block (All Parsing Functions)\n",
    "#   Paste this in Cell 1 and run once after kernel restart\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Project paths (edit if needed)\n",
    "# ------------------------------------------------------------\n",
    "PROJECT_DIR = Path(\"/Users/peyma/Desktop/PROJECT 24-0048 (CII Prototype Strain Bioreactor Validation)\")\n",
    "OUTPUT_DIR  = PROJECT_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PRIORITY = {\n",
    "    \"24-0048-EXP1.xlsx\",\n",
    "    \"24-0048-EXP3.xlsx\",\n",
    "    \"24-0048-EXP4.xlsx\",\n",
    "    \"24-0048-EXP5.xlsx\",\n",
    "    \"data_compilation.xlsx\",\n",
    "}\n",
    "PROCESS_LIMIT = 50   # safe upper limit\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Utility: find Excel files, prioritize EXP decks\n",
    "# ------------------------------------------------------------\n",
    "def iter_excels(root: Path, priority_names: set, limit: int):\n",
    "    files = []\n",
    "    for r, _, fs in os.walk(root):\n",
    "        for f in fs:\n",
    "            if f.lower().endswith((\".xlsx\", \".xls\")):\n",
    "                files.append(Path(r) / f)\n",
    "\n",
    "    # sort: EXP decks first, then others\n",
    "    files = sorted(\n",
    "        files,\n",
    "        key=lambda p: (0 if p.name in priority_names else 1, p.name)\n",
    "    )\n",
    "    return files[:limit]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Utility: normalize any timestamp ‚Üí hours\n",
    "# ------------------------------------------------------------\n",
    "def get_time_hours(series):\n",
    "    s = series\n",
    "\n",
    "    # Numeric timestamps ‚Üí use directly\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    # Proper datetimes\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    if dt.notna().sum() >= 2:\n",
    "        return (dt - dt.min()) / np.timedelta64(1, \"h\")\n",
    "\n",
    "    # Try numeric coercion\n",
    "    num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if num.notna().sum() >= 2:\n",
    "        return num.astype(float)\n",
    "\n",
    "    # final fallback: just use row index\n",
    "    return pd.Series(np.arange(len(s)), index=s.index, dtype=float)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Header detection helpers\n",
    "# ------------------------------------------------------------\n",
    "re_do_pv = re.compile(r\"DO\\s*\\d+\\.PV\\s*\\[\\s*%DO\\s*\\]\", re.IGNORECASE)\n",
    "re_do_sp = re.compile(r\"DO\\s*\\d+\\.SP\\s*\\[\\s*%DO\\s*\\]\", re.IGNORECASE)\n",
    "re_fa_pv = re.compile(r\"FA\\s*\\d+\\.PV\\s*\\[\\s*mL/h\\s*\\]\", re.IGNORECASE)\n",
    "\n",
    "def detect_col(cols, tests):\n",
    "    \"\"\"Return first column name matching any provided test.\"\"\"\n",
    "    for t in tests:\n",
    "        for c in cols:\n",
    "            if t(c):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Parse the GROWTH sheet\n",
    "# ------------------------------------------------------------\n",
    "def parse_growth(xf):\n",
    "    sh = next((s for s in xf.sheet_names if s.strip().lower() == 'growth'), None)\n",
    "    if sh is None:\n",
    "        return None\n",
    "\n",
    "    df = xf.parse(sh)\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "    # time\n",
    "    time_col = detect_col(cols, [\n",
    "        lambda c: any(k in c.lower() for k in [\"time\", \"timestamp\", \"duration\", \"date\"])\n",
    "    ])\n",
    "\n",
    "    # OD\n",
    "    od_col = detect_col(cols, [\n",
    "        lambda c: \"od600\" in c.lower()\n",
    "    ])\n",
    "\n",
    "    # DO sensors\n",
    "    do_pv_col = detect_col(cols, [lambda c: re_do_pv.search(c)])\n",
    "    do_sp_col = detect_col(cols, [lambda c: re_do_sp.search(c)])\n",
    "\n",
    "    # feed\n",
    "    fa_pv_col = detect_col(cols, [lambda c: re_fa_pv.search(c)])\n",
    "\n",
    "    t = get_time_hours(df[time_col]) if time_col else pd.Series(np.arange(len(df)))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"time_hours\": t,\n",
    "        \"od600\": pd.to_numeric(df[od_col], errors=\"coerce\") if od_col else np.nan,\n",
    "        \"do_pv\": pd.to_numeric(df[do_pv_col], errors=\"coerce\") if do_pv_col else np.nan,\n",
    "        \"do_sp\": pd.to_numeric(df[do_sp_col], errors=\"coerce\") if do_sp_col else np.nan,\n",
    "        \"feed_ml_h\": pd.to_numeric(df[fa_pv_col], errors=\"coerce\") if fa_pv_col else np.nan,\n",
    "    })\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Parse Data1/2/3/4 (process logs)\n",
    "# ------------------------------------------------------------\n",
    "def parse_data(xf):\n",
    "    sheets = [s for s in xf.sheet_names if s.lower().startswith(\"data\")]\n",
    "    if not sheets:\n",
    "        return None\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for sh in sheets:\n",
    "        try:\n",
    "            df = xf.parse(sh, header=None)\n",
    "\n",
    "            # row 1 contains real headers\n",
    "            df.columns = df.iloc[1].astype(str).str.strip()\n",
    "            df = df.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "            cols = df.columns\n",
    "\n",
    "            # ------------------------------\n",
    "            # TIME column\n",
    "            # ------------------------------\n",
    "            if \"InoculationTime []\" in cols:\n",
    "                def to_hours(val):\n",
    "                    if pd.isna(val):\n",
    "                        return np.nan\n",
    "                    if isinstance(val, (int, float)):\n",
    "                        return float(val) * 24.0\n",
    "                    try:\n",
    "                        td = pd.to_timedelta(str(val).split(\".\")[0])\n",
    "                        return td.total_seconds() / 3600\n",
    "                    except:\n",
    "                        return np.nan\n",
    "\n",
    "                time_hours = df[\"InoculationTime []\"].apply(to_hours)\n",
    "\n",
    "            elif \"Timestamp\" in cols:\n",
    "                dt = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "                time_hours = (dt - dt.min()) / np.timedelta64(1, \"h\")\n",
    "\n",
    "            else:\n",
    "                time_hours = np.arange(len(df))\n",
    "\n",
    "            # ------------------------------\n",
    "            # PROCESS parameters\n",
    "            # ------------------------------\n",
    "            col_do     = \"DO1.PV [%DO]\"\n",
    "            col_air    = \"F1.PV [sL/h]\"\n",
    "            col_fa     = \"FA1.PV [mL/h]\"\n",
    "            col_rpm    = \"N1.PV [rpm]\"\n",
    "            col_ph     = \"pH1.PV [pH]\"\n",
    "            col_temp   = \"T1.PV [¬∞C]\"\n",
    "            col_fb     = \"FB1.PV [mL/h]\"\n",
    "            col_fc     = \"FC1.PV [mL/h]\"\n",
    "\n",
    "            out = pd.DataFrame({\n",
    "                \"time_hours\": time_hours,\n",
    "                \"do_data\":      pd.to_numeric(df.get(col_do, np.nan), errors=\"coerce\"),\n",
    "                \"air_slph\":     pd.to_numeric(df.get(col_air, np.nan), errors=\"coerce\"),\n",
    "                \"feed_ml_h\":    pd.to_numeric(df.get(col_fa, np.nan), errors=\"coerce\"),\n",
    "                \"rpm\":          pd.to_numeric(df.get(col_rpm, np.nan), errors=\"coerce\"),\n",
    "                \"ph\":           pd.to_numeric(df.get(col_ph, np.nan), errors=\"coerce\"),\n",
    "                \"temp_c\":       pd.to_numeric(df.get(col_temp, np.nan), errors=\"coerce\"),\n",
    "                \"base_ml_h\":    pd.to_numeric(df.get(col_fb, np.nan), errors=\"coerce\"),\n",
    "                \"antifoam_ml_h\":pd.to_numeric(df.get(col_fc, np.nan), errors=\"coerce\"),\n",
    "                \"source_sheet\": sh,\n",
    "            })\n",
    "\n",
    "            all_rows.append(out)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error parsing {sh}: {e}\")\n",
    "\n",
    "    if not all_rows:\n",
    "        return None\n",
    "\n",
    "    return pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Parse HPLC (future: glycerol / methanol)\n",
    "# ------------------------------------------------------------\n",
    "def parse_hplc(xf):\n",
    "    sh = next((s for s in xf.sheet_names if \"hplc\" in s.lower()), None)\n",
    "    if sh is None:\n",
    "        return None\n",
    "\n",
    "    df = xf.parse(sh)\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "    time_col = detect_col(cols, [lambda c: \"time\" in c.lower()])\n",
    "    t = get_time_hours(df[time_col]) if time_col else np.arange(len(df))\n",
    "\n",
    "    gly = detect_col(cols, [lambda c: \"glycerol\" in c.lower()])\n",
    "    meoh = detect_col(cols, [lambda c: \"methanol\" in c.lower() or \"meoh\" in c.lower()])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"time_hours\": t,\n",
    "        \"glycerol\": pd.to_numeric(df[gly], errors=\"coerce\") if gly else np.nan,\n",
    "        \"methanol\": pd.to_numeric(df[meoh], errors=\"coerce\") if meoh else np.nan,\n",
    "    })\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Parse data_compilation.xlsx (collagen ‚Äì not used yet)\n",
    "# ------------------------------------------------------------\n",
    "def parse_compilation(xf, name):\n",
    "    if \"data_compilation.xlsx\" not in name.lower():\n",
    "        return None\n",
    "\n",
    "    df = xf.parse(xf.sheet_names[0])\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    cols = df.columns.tolist()\n",
    "\n",
    "    time_col = detect_col(cols, [lambda c: \"time\" in c.lower()])\n",
    "    t = get_time_hours(df[time_col]) if time_col else np.arange(len(df))\n",
    "\n",
    "    coll_col = detect_col(cols, [lambda c: \"collagen\" in c.lower() or \"titer\" in c.lower()])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"time_hours\": t,\n",
    "        \"collagen\": pd.to_numeric(df[coll_col], errors=\"coerce\") if coll_col else np.nan,\n",
    "    })\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Parse Feeding sheets (sum total feed volume)\n",
    "# ------------------------------------------------------------\n",
    "def parse_feeding(xf):\n",
    "    feed_sheets = [s for s in xf.sheet_names if \"feed\" in s.lower()]\n",
    "    totals = []\n",
    "\n",
    "    for sh in feed_sheets:\n",
    "        try:\n",
    "            df = xf.parse(sh)\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # detect numeric feed columns\n",
    "            cand = [c for c in df.columns if re.search(r\"(total|used|consumed)\", str(c), re.I)]\n",
    "            if cand:\n",
    "                sub = pd.to_numeric(df[cand], errors=\"coerce\")\n",
    "                val = np.nansum(sub.values)\n",
    "                if np.isfinite(val) and val > 0:\n",
    "                    totals.append(val)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not totals:\n",
    "        return None\n",
    "    return max(totals)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Merge-as-of utility\n",
    "# ------------------------------------------------------------\n",
    "def merge_asof_per_file(base: pd.DataFrame, add: pd.DataFrame, cols, tol=0.5):\n",
    "    if base is None or add is None:\n",
    "        return base\n",
    "\n",
    "    if \"time_hours\" not in base or \"time_hours\" not in add:\n",
    "        return base\n",
    "\n",
    "    l = base.dropna(subset=[\"time_hours\"]).sort_values(\"time_hours\")\n",
    "    r = add.dropna(subset=[\"time_hours\"]).sort_values(\"time_hours\")\n",
    "\n",
    "    m = pd.merge_asof(\n",
    "        l,\n",
    "        r[[\"time_hours\"] + cols],\n",
    "        on=\"time_hours\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=tol\n",
    "    )\n",
    "    return m\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Full loader: build master dataframe\n",
    "# ------------------------------------------------------------\n",
    "def build_master(root_dir: Path):\n",
    "    files = iter_excels(root_dir, PRIORITY, PROCESS_LIMIT)\n",
    "    master_list = []\n",
    "\n",
    "    for p in files:\n",
    "        try:\n",
    "            xf = pd.ExcelFile(p)\n",
    "            g = parse_growth(xf)\n",
    "            d = parse_data(xf)\n",
    "            h = parse_hplc(xf)\n",
    "            c = parse_compilation(xf, p.name)\n",
    "            ftotal = parse_feeding(xf)\n",
    "\n",
    "            # choose first available as base\n",
    "            base = None\n",
    "            for comp in [d, g]:\n",
    "                if isinstance(comp, pd.DataFrame):\n",
    "                    base = comp.copy()\n",
    "                    break\n",
    "\n",
    "            if base is None:\n",
    "                print(f\"‚ö†Ô∏è No usable sheets in {p.name}\")\n",
    "                continue\n",
    "\n",
    "            # merge optional sheets\n",
    "            if g is not None and base is not g:\n",
    "                base = merge_asof_per_file(base, g, [\"od600\", \"do_pv\", \"do_sp\", \"feed_ml_h\"])\n",
    "\n",
    "            if h is not None:\n",
    "                base = merge_asof_per_file(base, h, [\"glycerol\", \"methanol\"])\n",
    "\n",
    "            if c is not None:\n",
    "                base = merge_asof_per_file(base, c, [\"collagen\"])\n",
    "\n",
    "            if ftotal is not None:\n",
    "                base[\"feed_total_used\"] = ftotal\n",
    "\n",
    "            # annotate file\n",
    "            base[\"source_file\"] = p.name\n",
    "            master_list.append(base)\n",
    "\n",
    "            print(f\"Loaded {p.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {p.name}: {e}\")\n",
    "\n",
    "    if not master_list:\n",
    "        print(\"‚ùå No runs detected.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    master = pd.concat(master_list, ignore_index=True)\n",
    "\n",
    "    # add exp_id (EX: EXP3)\n",
    "    master[\"exp_id\"] = master[\"source_file\"].str.extract(r\"(EXP\\d+)\", expand=False)\n",
    "\n",
    "    return master\n",
    "\n",
    "# ============================================================\n",
    "# END OF HELPER BLOCK\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d97686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR: /Users/peyma/Desktop/PROJECT 24-0048 (CII Prototype Strain Bioreactor Validation)\n",
      "OUTPUT_DIR: /Users/peyma/Desktop/PROJECT 24-0048 (CII Prototype Strain Bioreactor Validation)/outputs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ================================\n",
    "# Cell 1 ‚Äì Imports & paths\n",
    "# ================================\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- CHANGE THIS to your real project folder if needed -----\n",
    "PROJECT_DIR = Path(\"/Users/peyma/Desktop/PROJECT 24-0048 (CII Prototype Strain Bioreactor Validation)\")\n",
    "\n",
    "OUTPUT_DIR = PROJECT_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a478899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 2 ‚Äì General helpers & utilities\n",
    "# ================================\n",
    "\n",
    "def iter_exp_excels(root: Path):\n",
    "    \"\"\"\n",
    "    Return all Excel files that look like EXP summary decks (24-0048-EXP*.xlsx or Liven Batch *_EXP* Summary.xlsx).\n",
    "    Ignore SDS, raw data, gels, DSP, analysis etc.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for p in root.rglob(\"*.xlsx\"):\n",
    "        name_l = p.name.lower()\n",
    "        if any(bad in name_l for bad in [\"sds\", \"raw data\", \"gel\", \"analysis\", \"dsp\"]):\n",
    "            continue\n",
    "        if (\"exp\" in name_l) and (name_l.endswith(\".xlsx\")):\n",
    "            files.append(p)\n",
    "    files = sorted(files)\n",
    "    return files\n",
    "\n",
    "\n",
    "def extract_exp_id(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Try to get EXP id (EXP1, EXP3, ...) from file name.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"exp\\s*([0-9]+)\", path.name, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return f\"EXP{int(m.group(1))}\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "def duration_to_hours(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert 'Duration' or similar columns to hours.\n",
    "    Handles:\n",
    "      - Excel time strings: '0:05:30', '00:05:30'\n",
    "      - Timedelta strings with microseconds\n",
    "      - Numeric fraction-of-day (Excel)\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    # if numeric: assume Excel fraction of a day\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return pd.to_numeric(s, errors=\"coerce\") * 24.0\n",
    "\n",
    "    out = []\n",
    "    for v in s:\n",
    "        if pd.isna(v):\n",
    "            out.append(np.nan)\n",
    "            continue\n",
    "        # numeric embedded as string\n",
    "        try:\n",
    "            fv = float(v)\n",
    "            out.append(fv * 24.0)\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        v_str = str(v).strip()\n",
    "        try:\n",
    "            # remove microseconds part if present\n",
    "            v_main = v_str.split(\".\")[0]\n",
    "            td = pd.to_timedelta(v_main)\n",
    "            out.append(td.total_seconds() / 3600.0)\n",
    "        except Exception:\n",
    "            out.append(np.nan)\n",
    "\n",
    "    return pd.Series(out, index=s.index, dtype=float)\n",
    "\n",
    "\n",
    "def safe_numeric(series, default=np.nan):\n",
    "    if series is None:\n",
    "        return pd.Series(default, index=pd.RangeIndex(0))\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "203d6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 3 ‚Äì parse Data1‚ÄìData4\n",
    "# ================================\n",
    "\n",
    "def parse_data_sheets(xf: pd.ExcelFile, source_file: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Parse Data1‚ÄìData4 sheets from a DASGIP EXP deck.\n",
    "    Uses Duration as time_hours (converted to hours).\n",
    "    Each sheet is treated as one vessel (vessel_id = sheet name).\n",
    "    \"\"\"\n",
    "    data_sheets = [s for s in xf.sheet_names if s.strip().lower().startswith(\"data\")]\n",
    "    if not data_sheets:\n",
    "        return None\n",
    "\n",
    "    exp_id = extract_exp_id(source_file)\n",
    "    all_rows = []\n",
    "\n",
    "    for sh in data_sheets:\n",
    "        try:\n",
    "            raw = xf.parse(sh, header=None)\n",
    "\n",
    "            # Find the header row by looking for \"Timestamp\"\n",
    "            header_row = None\n",
    "            for i in range(min(10, len(raw))):\n",
    "                row_vals = raw.iloc[i].astype(str).str.strip()\n",
    "                if row_vals.str.contains(\"Timestamp\", case=False, na=False).any():\n",
    "                    header_row = i\n",
    "                    break\n",
    "\n",
    "            if header_row is None:\n",
    "                # fallback: assume row 1 is header\n",
    "                header_row = 1\n",
    "\n",
    "            df = raw.copy()\n",
    "            df.columns = df.iloc[header_row].astype(str).str.strip()\n",
    "            df = df.iloc[header_row + 1 :].reset_index(drop=True)\n",
    "\n",
    "            # time column: prefer Duration, then InoculationTime, else Timestamp\n",
    "            if \"Duration\" in df.columns:\n",
    "                t = duration_to_hours(df[\"Duration\"])\n",
    "            elif \"InoculationTime []\" in df.columns:\n",
    "                t = duration_to_hours(df[\"InoculationTime []\"])\n",
    "            elif \"Timestamp\" in df.columns:\n",
    "                dt = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "                t = (dt - dt.min()) / np.timedelta64(1, \"h\")\n",
    "            else:\n",
    "                t = pd.Series(np.arange(len(df)), dtype=float)\n",
    "\n",
    "            out = pd.DataFrame({\n",
    "                \"time_hours\": t,\n",
    "                \"do_data\": safe_numeric(df.get(\"DO1.PV [%DO]\")),\n",
    "                \"air_slph\": safe_numeric(df.get(\"F1.PV [sL/h]\")),\n",
    "                \"feed_fa\": safe_numeric(df.get(\"FA1.PV [mL/h]\")),  # glycerol / MeOH\n",
    "                \"feed_fb\": safe_numeric(df.get(\"FB1.PV [mL/h]\")),  # base\n",
    "                \"feed_fc\": safe_numeric(df.get(\"FC1.PV [mL/h]\")),  # antifoam\n",
    "                \"rpm\": safe_numeric(df.get(\"N1.PV [rpm]\")),\n",
    "                \"ph\": safe_numeric(df.get(\"pH1.PV [pH]\")),\n",
    "                \"temp_c\": safe_numeric(df.get(\"T1.PV [¬∞C]\")),\n",
    "            })\n",
    "\n",
    "            out[\"vessel_id\"] = sh.strip()      # e.g. \"Data1\"\n",
    "            out[\"exp_id\"] = exp_id\n",
    "            out[\"source_file\"] = source_file.name\n",
    "\n",
    "            all_rows.append(out)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error parsing {source_file.name} / {sh}: {e}\")\n",
    "\n",
    "    if not all_rows:\n",
    "        return None\n",
    "    df_all = pd.concat(all_rows, ignore_index=True)\n",
    "    # Drop rows where time is missing completely\n",
    "    return df_all.dropna(subset=[\"time_hours\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2f34ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 4 ‚Äì parse Growth (OD600)\n",
    "# ================================\n",
    "\n",
    "def parse_growth_sheet(xf: pd.ExcelFile, source_file: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Parse Growth sheet for OD600 vs time.\n",
    "    Assumes one time column + multiple OD columns (possibly per vessel).\n",
    "    \"\"\"\n",
    "    growth_name = next((s for s in xf.sheet_names if s.strip().lower() == \"growth\"), None)\n",
    "    if growth_name is None:\n",
    "        return None\n",
    "\n",
    "    df = xf.parse(growth_name)\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    # time column\n",
    "    time_col = None\n",
    "    for c in cols:\n",
    "        c_low = c.lower()\n",
    "        if any(k in c_low for k in [\"time\", \"duration\", \"hour\", \"hr\"]):\n",
    "            time_col = c\n",
    "            break\n",
    "\n",
    "    if time_col is None:\n",
    "        # fall back to first column\n",
    "        time_col = cols[0]\n",
    "\n",
    "    t = duration_to_hours(df[time_col])\n",
    "\n",
    "    # OD columns\n",
    "    od_cols = [c for c in cols if \"od600\" in c.lower() or re.search(r\"\\bod\\s*600\\b\", c, flags=re.IGNORECASE)]\n",
    "    if not od_cols:\n",
    "        return None\n",
    "\n",
    "    exp_id = extract_exp_id(source_file)\n",
    "    rows = []\n",
    "\n",
    "    for col in od_cols:\n",
    "        od = safe_numeric(df[col])\n",
    "        vessel_id = col  # often something like \"OD600 Data1\" etc.\n",
    "\n",
    "        tmp = pd.DataFrame({\n",
    "            \"time_hours\": t,\n",
    "            \"od600\": od,\n",
    "            \"vessel_id\": vessel_id,\n",
    "            \"exp_id\": exp_id,\n",
    "            \"source_file\": source_file.name,\n",
    "        })\n",
    "        rows.append(tmp)\n",
    "\n",
    "    if not rows:\n",
    "        return None\n",
    "    return pd.concat(rows, ignore_index=True).dropna(subset=[\"time_hours\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7790d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 5 EXP-like Excel files\n",
      " - 24-0048-EXP1 (Lambton)/24-0048-EXP1.xlsx\n",
      " - 24-0048-EXP1 (Lambton)/~$24-0048-EXP1.xlsx\n",
      " - 24-0048-EXP3 (Lambton)/24-0048-EXP3.xlsx\n",
      " - 24-0048-EXP4 (Lambton) comparing BMG and MP on CII constructs, DASGIP/Liven Batch 24-0048-EXP4 Summary.xlsx\n",
      " - 24-0048-EXP5 (Lambton) - repeat of EXP4/Liven Batch 24-0048-EXP5 Summary.xlsx\n",
      "\n",
      "üìò Reading 24-0048-EXP1.xlsx\n",
      "  ‚Üí Data sheets parsed: ['Data1' 'Data2' 'Data3' 'Data4']\n",
      "‚ùå Could not open /Users/peyma/Desktop/PROJECT 24-0048 (CII Prototype Strain Bioreactor Validation)/24-0048-EXP1 (Lambton)/~$24-0048-EXP1.xlsx: Excel file format cannot be determined, you must specify an engine manually.\n",
      "\n",
      "üìò Reading 24-0048-EXP3.xlsx\n",
      "  ‚Üí Data sheets parsed: ['Data1' 'Data2' 'Data3' 'Data4']\n",
      "\n",
      "üìò Reading Liven Batch 24-0048-EXP4 Summary.xlsx\n",
      "  ‚Üí Data sheets parsed: ['Data1' 'Data2' 'Data3' 'Data4']\n",
      "\n",
      "üìò Reading Liven Batch 24-0048-EXP5 Summary.xlsx\n",
      "  ‚Üí Data sheets parsed: ['DataA' 'DataB' 'DataC' 'DataD']\n",
      "\n",
      "‚ö†Ô∏è No Growth sheets parsed; master will have no OD600.\n",
      "\n",
      "Columns: ['time_hours', 'do_data', 'air_slph', 'feed_fa', 'feed_fb', 'feed_fc', 'rpm', 'ph', 'temp_c', 'vessel_id', 'exp_id', 'source_file', 'od600']\n",
      "   time_hours  do_data  air_slph  feed_fa  feed_fb  feed_fc  rpm    ph  \\\n",
      "0    0.008333   98.292   120.733      0.0      0.0      0.0  0.0  1.14   \n",
      "1    0.016667   91.941     0.000      0.0      0.0      0.0  0.0  1.14   \n",
      "2    0.025000   90.900     0.000      0.0      0.0      0.0  0.0  1.14   \n",
      "3    0.033333   90.109     0.000      0.0      0.0      0.0  0.0  1.14   \n",
      "4    0.041667   89.536     0.000      0.0      0.0      0.0  0.0  1.14   \n",
      "\n",
      "   temp_c vessel_id exp_id        source_file  od600  \n",
      "0  39.432     Data1   EXP1  24-0048-EXP1.xlsx    NaN  \n",
      "1  39.432     Data1   EXP1  24-0048-EXP1.xlsx    NaN  \n",
      "2  39.432     Data1   EXP1  24-0048-EXP1.xlsx    NaN  \n",
      "3  39.393     Data1   EXP1  24-0048-EXP1.xlsx    NaN  \n",
      "4  39.356     Data1   EXP1  24-0048-EXP1.xlsx    NaN  \n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Cell 5 ‚Äì Build master table\n",
    "# ================================\n",
    "\n",
    "def build_master(project_dir: Path) -> pd.DataFrame:\n",
    "    files = iter_exp_excels(project_dir)\n",
    "    print(f\"‚úÖ Found {len(files)} EXP-like Excel files\")\n",
    "    for p in files:\n",
    "        print(\" -\", p.relative_to(project_dir))\n",
    "\n",
    "    all_data = []\n",
    "    all_growth = []\n",
    "\n",
    "    for p in files:\n",
    "        try:\n",
    "            xf = pd.ExcelFile(p)\n",
    "            print(f\"\\nüìò Reading {p.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not open {p}: {e}\")\n",
    "            continue\n",
    "\n",
    "        d = parse_data_sheets(xf, p)\n",
    "        g = parse_growth_sheet(xf, p)\n",
    "\n",
    "        if d is not None:\n",
    "            all_data.append(d)\n",
    "            print(f\"  ‚Üí Data sheets parsed: {d['vessel_id'].unique()}\")\n",
    "\n",
    "        if g is not None:\n",
    "            all_growth.append(g)\n",
    "            print(f\"  ‚Üí Growth sheet parsed with {g['vessel_id'].nunique()} OD traces\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"\\n‚ùå No Data sheets parsed; master will be empty.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    if not all_growth:\n",
    "        print(\"\\n‚ö†Ô∏è No Growth sheets parsed; master will have no OD600.\")\n",
    "        master = data_all.copy()\n",
    "        master[\"od600\"] = np.nan\n",
    "        return master\n",
    "\n",
    "    growth_all = pd.concat(all_growth, ignore_index=True)\n",
    "\n",
    "    # --- Merge OD600 per exp_id + vessel, nearest time ---\n",
    "    merged_list = []\n",
    "    for (exp_id, vessel_id), df_group in data_all.groupby([\"exp_id\", \"vessel_id\"], as_index=False):\n",
    "        sub_data = df_group.sort_values(\"time_hours\")\n",
    "        # Try matching growth on same vessel_id; if none, match any from same exp\n",
    "        g_sub = growth_all[(growth_all[\"exp_id\"] == exp_id)]\n",
    "        g_same = g_sub[g_sub[\"vessel_id\"] == vessel_id]\n",
    "        if g_same.empty:\n",
    "            g_same = g_sub\n",
    "\n",
    "        if g_same.empty:\n",
    "            sub_data[\"od600\"] = np.nan\n",
    "            merged_list.append(sub_data)\n",
    "            continue\n",
    "\n",
    "        g_sorted = g_same.sort_values(\"time_hours\")\n",
    "        m = pd.merge_asof(\n",
    "            sub_data,\n",
    "            g_sorted[[\"time_hours\", \"od600\"]],\n",
    "            on=\"time_hours\",\n",
    "            direction=\"nearest\",\n",
    "            tolerance=0.25,  # hours\n",
    "        )\n",
    "        merged_list.append(m)\n",
    "\n",
    "    master = pd.concat(merged_list, ignore_index=True)\n",
    "    print(f\"\\n‚úÖ Master built with {len(master)} rows\")\n",
    "    return master\n",
    "\n",
    "\n",
    "# ---- Actually build it ----\n",
    "master = build_master(PROJECT_DIR)\n",
    "\n",
    "print(\"\\nColumns:\", master.columns.tolist())\n",
    "print(master.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f6ac5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Cell 6 ‚Äì Matplotlib plotting helpers\n",
    "# ================================\n",
    "\n",
    "def ensure_sorted(df):\n",
    "    return df.sort_values(\"time_hours\").dropna(subset=[\"time_hours\"])\n",
    "\n",
    "\n",
    "def plot_ph_temp(df_exp, exp_id, out_dir: Path):\n",
    "    df = ensure_sorted(df_exp)\n",
    "    if df[\"ph\"].notna().sum() == 0 and df[\"temp_c\"].notna().sum() == 0:\n",
    "        print(\"  ‚ö†Ô∏è No pH/Temp data.\")\n",
    "        return\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    for v, sub in df.groupby(\"vessel_id\"):\n",
    "        sub = ensure_sorted(sub)\n",
    "        if sub[\"ph\"].notna().sum() > 0:\n",
    "            ax1.plot(sub[\"time_hours\"], sub[\"ph\"], label=f\"{v} pH\")\n",
    "        if sub[\"temp_c\"].notna().sum() > 0:\n",
    "            ax2.plot(sub[\"time_hours\"], sub[\"temp_c\"], linestyle=\"--\", label=f\"{v} Temp\")\n",
    "\n",
    "    ax1.set_xlabel(\"Time (h)\")\n",
    "    ax1.set_ylabel(\"pH\")\n",
    "    ax2.set_ylabel(\"Temperature (¬∞C)\")\n",
    "    fig.suptitle(f\"{exp_id} ‚Äì pH & Temperature\")\n",
    "\n",
    "    # Build combined legend\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = out_dir / f\"{exp_id}_pH_temp.png\"\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(\"  ‚úî Saved:\", out_path.name)\n",
    "\n",
    "\n",
    "def plot_do_air_rpm(df_exp, exp_id, out_dir: Path):\n",
    "    df = ensure_sorted(df_exp)\n",
    "    if (df[\"do_data\"].notna().sum() == 0 and\n",
    "        df[\"air_slph\"].notna().sum() == 0 and\n",
    "        df[\"rpm\"].notna().sum() == 0):\n",
    "        print(\"  ‚ö†Ô∏è No DO/Air/RPM data.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    for v, sub in df.groupby(\"vessel_id\"):\n",
    "        sub = ensure_sorted(sub)\n",
    "        if sub[\"do_data\"].notna().sum() > 0:\n",
    "            ax.plot(sub[\"time_hours\"], sub[\"do_data\"], label=f\"{v} DO [%]\")\n",
    "        if sub[\"air_slph\"].notna().sum() > 0:\n",
    "            ax.plot(sub[\"time_hours\"], sub[\"air_slph\"], linestyle=\"--\", label=f\"{v} Air [sL/h]\")\n",
    "        if sub[\"rpm\"].notna().sum() > 0:\n",
    "            ax.plot(sub[\"time_hours\"], sub[\"rpm\"], linestyle=\":\", label=f\"{v} RPM\")\n",
    "\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.set_ylabel(\"Value (DO %, sL/h, rpm)\")\n",
    "    ax.set_title(f\"{exp_id} ‚Äì DO, Air & Agitation\")\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = out_dir / f\"{exp_id}_DO_Air_RPM.png\"\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(\"  ‚úî Saved:\", out_path.name)\n",
    "\n",
    "\n",
    "def plot_feeds(df_exp, exp_id, out_dir: Path):\n",
    "    df = ensure_sorted(df_exp)\n",
    "\n",
    "    # ---- FA1: glycerol / MeOH ----\n",
    "    if df[\"feed_fa\"].notna().sum() > 0:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        for v, sub in df.groupby(\"vessel_id\"):\n",
    "            sub = ensure_sorted(sub)\n",
    "            ax.plot(sub[\"time_hours\"], sub[\"feed_fa\"], label=v)\n",
    "\n",
    "        ax.set_xlabel(\"Time (h)\")\n",
    "        ax.set_ylabel(\"FA1 [mL/h]\")\n",
    "        ax.set_title(f\"{exp_id} ‚Äì Feed FA1 (Glycerol pre-induction, MeOH post-induction)\")\n",
    "        ax.legend(title=\"Vessel\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "        fig.tight_layout()\n",
    "        out_path = out_dir / f\"{exp_id}_Feed_FA1.png\"\n",
    "        fig.savefig(out_path, dpi=200)\n",
    "        plt.close(fig)\n",
    "        print(\"  ‚úî Saved:\", out_path.name)\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No FA1 feed data.\")\n",
    "\n",
    "    # ---- FB1: base ----\n",
    "    if df[\"feed_fb\"].notna().sum() > 0:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        for v, sub in df.groupby(\"vessel_id\"):\n",
    "            sub = ensure_sorted(sub)\n",
    "            ax.plot(sub[\"time_hours\"], sub[\"feed_fb\"], label=v)\n",
    "\n",
    "        ax.set_xlabel(\"Time (h)\")\n",
    "        ax.set_ylabel(\"FB1 [mL/h]\")\n",
    "        ax.set_title(f\"{exp_id} ‚Äì Base Feed (FB1)\")\n",
    "        ax.legend(title=\"Vessel\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "        fig.tight_layout()\n",
    "        out_path = out_dir / f\"{exp_id}_Feed_FB1_base.png\"\n",
    "        fig.savefig(out_path, dpi=200)\n",
    "        plt.close(fig)\n",
    "        print(\"  ‚úî Saved:\", out_path.name)\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No FB1 base feed data.\")\n",
    "\n",
    "    # ---- FC1: antifoam ----\n",
    "    if df[\"feed_fc\"].notna().sum() > 0:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        for v, sub in df.groupby(\"vessel_id\"):\n",
    "            sub = ensure_sorted(sub)\n",
    "            ax.plot(sub[\"time_hours\"], sub[\"feed_fc\"], label=v)\n",
    "\n",
    "        ax.set_xlabel(\"Time (h)\")\n",
    "        ax.set_ylabel(\"FC1 [mL/h]\")\n",
    "        ax.set_title(f\"{exp_id} ‚Äì Antifoam Feed (FC1)\")\n",
    "        ax.legend(title=\"Vessel\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "        fig.tight_layout()\n",
    "        out_path = out_dir / f\"{exp_id}_Feed_FC1_antifoam.png\"\n",
    "        fig.savefig(out_path, dpi=200)\n",
    "        plt.close(fig)\n",
    "        print(\"  ‚úî Saved:\", out_path.name)\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No FC1 antifoam feed data.\")\n",
    "\n",
    "\n",
    "def plot_od(df_exp, exp_id, out_dir: Path):\n",
    "    if \"od600\" not in df_exp.columns:\n",
    "        print(\"  ‚ö†Ô∏è No OD600 column in master.\")\n",
    "        return\n",
    "\n",
    "    df = ensure_sorted(df_exp)\n",
    "    if df[\"od600\"].notna().sum() == 0:\n",
    "        print(\"  ‚ö†Ô∏è OD600 data is all NaN.\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for v, sub in df.groupby(\"vessel_id\"):\n",
    "        sub = ensure_sorted(sub)\n",
    "        ax.plot(sub[\"time_hours\"], sub[\"od600\"], label=v)\n",
    "\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.set_ylabel(\"OD600\")\n",
    "    ax.set_title(f\"{exp_id} ‚Äì OD600 (Growth)\")\n",
    "    ax.legend(title=\"Vessel\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = out_dir / f\"{exp_id}_OD600.png\"\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(\"  ‚úî Saved:\", out_path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d888ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# Cell 7 ‚Äì Plotting helper functions\n",
    "# =====================================\n",
    "\n",
    "def plot_ph_temp(df_exp: pd.DataFrame, exp_id: str, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Overlay pH and Temperature for all vessels in one EXP.\n",
    "    \"\"\"\n",
    "    if df_exp[\"ph\"].notna().sum() == 0 and df_exp[\"temp_c\"].notna().sum() == 0:\n",
    "        print(f\"‚ö†Ô∏è No pH or temperature data for {exp_id}\")\n",
    "        return\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    for vessel, sub in df_exp.groupby(\"vessel\"):\n",
    "        sub = sub.sort_values(\"time_hours\")\n",
    "        if sub[\"ph\"].notna().any():\n",
    "            ax1.plot(sub[\"time_hours\"], sub[\"ph\"], label=f\"{vessel} pH\")\n",
    "        if sub[\"temp_c\"].notna().any():\n",
    "            ax2.plot(sub[\"time_hours\"], sub[\"temp_c\"], linestyle=\"--\", label=f\"{vessel} T\")\n",
    "\n",
    "    ax1.set_xlabel(\"Time (h)\")\n",
    "    ax1.set_ylabel(\"pH\")\n",
    "    ax2.set_ylabel(\"Temperature (¬∞C)\")\n",
    "    ax1.set_title(f\"{exp_id} ‚Äì pH & Temperature (all vessels)\")\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = out_dir / f\"{exp_id}_ph_temp.png\"\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(\"  üìà Saved:\", out_path.name)\n",
    "\n",
    "\n",
    "def plot_do_air_rpm(df_exp: pd.DataFrame, exp_id: str, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Overlay DO, air, and RPM for all vessels.\n",
    "    \"\"\"\n",
    "    if (\n",
    "        df_exp[\"do_data\"].notna().sum() == 0\n",
    "        and df_exp[\"air_slph\"].notna().sum() == 0\n",
    "        and df_exp[\"rpm\"].notna().sum() == 0\n",
    "    ):\n",
    "        print(f\"‚ö†Ô∏è No DO/air/rpm data for {exp_id}\")\n",
    "        return\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    for vessel, sub in df_exp.groupby(\"vessel\"):\n",
    "        sub = sub.sort_values(\"time_hours\")\n",
    "        if sub[\"do_data\"].notna().any():\n",
    "            ax1.plot(sub[\"time_hours\"], sub[\"do_data\"], label=f\"{vessel} DO%\")\n",
    "        if sub[\"air_slph\"].notna().any():\n",
    "            ax1.plot(sub[\"time_hours\"], sub[\"air_slph\"], linestyle=\"--\", label=f\"{vessel} Air (sL/h)\")\n",
    "        if sub[\"rpm\"].notna().any():\n",
    "            ax2.plot(sub[\"time_hours\"], sub[\"rpm\"], linestyle=\"-.\", label=f\"{vessel} RPM\")\n",
    "\n",
    "    ax1.set_xlabel(\"Time (h)\")\n",
    "    ax1.set_ylabel(\"DO [%] / Air [sL/h]\")\n",
    "    ax2.set_ylabel(\"Agitation [rpm]\")\n",
    "    ax1.set_title(f\"{exp_id} ‚Äì DO, Air & RPM (all vessels)\")\n",
    "\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"best\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = out_dir / f\"{exp_id}_do_air_rpm.png\"\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(\"  üìà Saved:\", out_path.name)\n",
    "\n",
    "\n",
    "def plot_feeds(df_exp: pd.DataFrame, exp_id: str, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Overlay glycerol vs MeOH feeds (both from FA1, split by induction time).\n",
    "    \"\"\"\n",
    "    if df_exp[\"glycerol_feed\"].notna().sum() == 0 and df_exp[\"meoh_feed\"].notna().sum() == 0:\n",
    "        print(f\"‚ö†Ô∏è No feed data for {exp_id}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for vessel, sub in df_exp.groupby(\"vessel\"):\n",
    "        sub = sub.sort_values(\"time_hours\")\n",
    "        if sub[\"glycerol_feed\"].notna().any():\n",
    "            ax.plot(sub[\"time_hours\"], sub[\"glycerol_feed\"], label=f\"{vessel} Glycerol\", linestyle=\"-\")\n",
    "        if sub[\"meoh_feed\"].notna().any():\n",
    "            ax.plot(sub[\"time_hours\"], sub[\"meoh_feed\"], label=f\"{vessel} MeOH\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.set_ylabel(\"FA1 feed (mL/h)\")\n",
    "    ax.set_title(f\"{exp_id} ‚Äì Glycerol & Methanol Feed (FA1, all vessels)\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = out_dir / f\"{exp_id}_feeds_fa.png\"\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(\"  üìà Saved:\", out_path.name)\n",
    "\n",
    "\n",
    "def plot_base(df_exp: pd.DataFrame, exp_id: str, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Overlay base (FB1) feed for all vessels.\n",
    "    \"\"\"\n",
    "    if df_exp[\"base_ml_h\"].notna().sum() == 0:\n",
    "        print(f\"‚ö†Ô∏è No base feed data for {exp_id}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    for vessel, sub in df_exp.groupby(\"vessel\"):\n",
    "        sub = sub.sort_values(\"time_hours\")\n",
    "        ax.plot(sub[\"time_hours\"], sub[\"base_ml_h\"], label=vessel)\n",
    "\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.set_ylabel(\"Base FB1 (mL/h)\")\n",
    "    ax.set_title(f\"{exp_id} ‚Äì Base Feed (FB1, all vessels)\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = out_dir / f\"{exp_id}_base_fb.png\"\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(\"  üìà Saved:\", out_path.name)\n",
    "\n",
    "\n",
    "def plot_antifoam(df_exp: pd.DataFrame, exp_id: str, out_dir: Path):\n",
    "    \"\"\"\n",
    "    Overlay antifoam (FC1) feed for all vessels.\n",
    "    \"\"\"\n",
    "    if df_exp[\"antifoam_ml_h\"].notna().sum() == 0:\n",
    "        print(f\"‚ö†Ô∏è No antifoam feed data for {exp_id}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    for vessel, sub in df_exp.groupby(\"vessel\"):\n",
    "        sub = sub.sort_values(\"time_hours\")\n",
    "        ax.plot(sub[\"time_hours\"], sub[\"antifoam_ml_h\"], label=vessel)\n",
    "\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.set_ylabel(\"Antifoam FC1 (mL/h)\")\n",
    "    ax.set_title(f\"{exp_id} ‚Äì Antifoam Feed (FC1, all vessels)\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = out_dir / f\"{exp_id}_antifoam_fc.png\"\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(\"  üìà Saved:\", out_path.name)\n",
    "\n",
    "\n",
    "def plot_od(df_exp: pd.DataFrame, exp_id: str, out_dir: Path):\n",
    "    \"\"\"\n",
    "    OD600 vs time for the experiment (not per vessel ‚Äî based on Growth sheet).\n",
    "    \"\"\"\n",
    "    if \"od600\" not in df_exp.columns or df_exp[\"od600\"].notna().sum() == 0:\n",
    "        print(f\"‚ö†Ô∏è No OD600 data for {exp_id}\")\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    sub = df_exp.dropna(subset=[\"od600\"]).sort_values(\"time_hours\")\n",
    "\n",
    "    ax.scatter(sub[\"time_hours\"], sub[\"od600\"], s=20)\n",
    "    ax.set_xlabel(\"Time (h)\")\n",
    "    ax.set_ylabel(\"OD600\")\n",
    "    ax.set_title(f\"{exp_id} ‚Äì OD600 (Growth)\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = out_dir / f\"{exp_id}_od600.png\"\n",
    "    fig.savefig(out_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(\"  üìà Saved:\", out_path.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "361c421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available experiments:\n",
      "  1. EXP1\n",
      "  2. EXP3\n",
      "  3. EXP4\n",
      "  4. EXP5\n",
      "‚ùå Experiment not found in master.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Cell 8 ‚Äì Ask user which EXP + which parameter groups\n",
    "# =====================================================\n",
    "\n",
    "def run_interactive(master: pd.DataFrame):\n",
    "    if master.empty:\n",
    "        print(\"‚ùå Master is empty; build it first with build_master(PROJECT_DIR).\")\n",
    "        return\n",
    "\n",
    "    exps = sorted(master[\"exp_id\"].unique())\n",
    "    print(\"\\nAvailable experiments:\")\n",
    "    for i, e in enumerate(exps, 1):\n",
    "        print(f\"  {i}. {e}\")\n",
    "\n",
    "    choice = input(\"Type experiment ID (e.g. EXP1) or index: \").strip()\n",
    "\n",
    "    if choice.isdigit() and 1 <= int(choice) <= len(exps):\n",
    "        exp_id = exps[int(choice) - 1]\n",
    "    else:\n",
    "        exp_id = choice.upper()\n",
    "\n",
    "    if exp_id not in exps:\n",
    "        print(\"‚ùå Experiment not found in master.\")\n",
    "        return\n",
    "\n",
    "    df_exp = master[master[\"exp_id\"] == exp_id].copy()\n",
    "\n",
    "    # Create subfolder per experiment\n",
    "    exp_out_dir = OUTPUT_DIR / exp_id\n",
    "    exp_out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    menu = {\n",
    "        \"1\": (\"pH & Temperature\",               plot_ph_temp),\n",
    "        \"2\": (\"DO, Air & RPM\",                  plot_do_air_rpm),\n",
    "        \"3\": (\"Feeds ‚Äì Glycerol & MeOH (FA1)\",  plot_feeds),\n",
    "        \"4\": (\"Base feed (FB1)\",                plot_base),\n",
    "        \"5\": (\"Antifoam feed (FC1)\",            plot_antifoam),\n",
    "        \"6\": (\"OD600 (Growth)\",                 plot_od),\n",
    "    }\n",
    "\n",
    "    print(\"\\nWhich parameter groups do you want to plot?\")\n",
    "    for k, (label, _) in menu.items(2):\n",
    "        print(f\"  {k}. {label}\")\n",
    "\n",
    "    picks = input(\"Enter numbers separated by commas (e.g. 1,2,3): \").replace(\" \", \"\").split(\",\")\n",
    "    picks = [p for p in picks if p in menu]\n",
    "\n",
    "    if not picks:\n",
    "        print(\"No valid selections; aborting.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nüìÇ Saving plots for {exp_id} into {exp_out_dir}\")\n",
    "\n",
    "    for p in picks:\n",
    "        label, func = menu[p]\n",
    "        print(f\"- {label} ‚Ä¶\")\n",
    "        func(df_exp, exp_id, exp_out_dir)\n",
    "\n",
    "    print(\"\\n‚úÖ Done. Check the PNGs inside:\", exp_out_dir)\n",
    "\n",
    "\n",
    "# üëâ Call this when master is built:\n",
    "run_interactive(master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1690d633",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EXTRACT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Extraction & Merge (run cell) ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m excels = iter_excels(\u001b[43mEXTRACT_DIR\u001b[49m, PRIORITY, PROCESS_LIMIT)\n\u001b[32m      3\u001b[39m runs = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m excels:\n",
      "\u001b[31mNameError\u001b[39m: name 'EXTRACT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# === Extraction & Merge (run cell) ===\n",
    "excels = iter_excels(EXTRACT_DIR, PRIORITY, PROCESS_LIMIT)\n",
    "runs = []\n",
    "for p in excels:\n",
    "    try:\n",
    "        xf = pd.ExcelFile(p)\n",
    "    except Exception:\n",
    "        continue\n",
    "    g = parse_growth(xf)\n",
    "    d = parse_data(xf)\n",
    "    h = parse_hplc(xf)\n",
    "    c = parse_compilation(xf, p.name)\n",
    "    feed_total = parse_feeding(xf)\n",
    "    if g is None:\n",
    "        continue\n",
    "    df = g.copy()\n",
    "def merge_asof_per_file(base: pd.DataFrame, add: pd.DataFrame, cols, tol=0.5):\n",
    "    if base is None or base.empty or add is None or add.empty:\n",
    "        return base\n",
    "\n",
    "    # Drop NAs and sort both\n",
    "    l = base.dropna(subset=['time_hours']).sort_values('time_hours')\n",
    "    r = add.dropna(subset=['time_hours']).sort_values('time_hours')\n",
    "\n",
    "    if l.empty or r.empty:\n",
    "        return base\n",
    "\n",
    "    # Ensure both are float type for merge_asof\n",
    "    l['time_hours'] = pd.to_numeric(l['time_hours'], errors='coerce').astype(float)\n",
    "    r['time_hours'] = pd.to_numeric(r['time_hours'], errors='coerce').astype(float)\n",
    "\n",
    "    try:\n",
    "        merged = pd.merge_asof(\n",
    "            l,\n",
    "            r[['time_hours'] + cols],\n",
    "            on='time_hours',\n",
    "            direction='nearest',\n",
    "            tolerance=tol\n",
    "        )\n",
    "        return merged\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Merge failed ({cols}): {e}\")\n",
    "        return base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edd163",
   "metadata": {},
   "source": [
    "**Artifacts**  \n",
    "- `/mnt/data/model_inputs_snapshot_template.csv`  \n",
    "- `/mnt/data/model_report_template.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9872d47d",
   "metadata": {},
   "source": [
    "# Suggestions & Optimization\n",
    "\n",
    "Data-driven levers from your models + literature-backed ranges for *Pichia pastoris* AOX1 induction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5716c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Suggestions for Pichia pastoris Collagen Fermentations (Liven Proteins) ===\n",
      "\n",
      "Upstream strategy (current template)\n",
      "------------------------------------\n",
      "‚Ä¢ Use glycerol batch ‚Üí derepression ‚Üí MeOH adaptation ‚Üí MeOH induction.\n",
      "‚Ä¢ FA1: glycerol before induction, methanol after (see Batch record 'Methanol fed started at ‚Ä¶ h').\n",
      "‚Ä¢ FB1: base feed for pH control.\n",
      "‚Ä¢ FC1: antifoam; monitor spikes as a proxy for foaming events.\n",
      "\n",
      "DO control\n",
      "----------\n",
      "‚Ä¢ Target DO setpoint 25‚Äì35% during MeOH induction.\n",
      "‚Ä¢ Avoid sustained DO < 15% for more than ~10 minutes (oxygen limitation, stress responses).\n",
      "‚Ä¢ Use airflow increase first, then agitation (rpm) in your DO cascade.\n",
      "‚Ä¢ Track specific oxygen uptake rate (sOUR) where possible as an internal 'demand' signal.\n",
      "\n",
      "Temperature\n",
      "-----------\n",
      "‚Ä¢ For collagen-like proteins, slightly lower temps often help folding and secretion.\n",
      "‚Ä¢ Typical induction window: 22‚Äì28 ¬∞C; a good starting point is 25‚Äì26 ¬∞C.\n",
      "‚Ä¢ Run a small DoE around 22, 25, 28 ¬∞C at otherwise matched conditions.\n",
      "\n",
      "pH & base usage\n",
      "---------------\n",
      "‚Ä¢ Maintain pH between 5.2 and 5.8 (start at 5.5).\n",
      "‚Ä¢ High base consumption + drifting pH may indicate:\n",
      "  ‚Äì overload of methanol,\n",
      "  ‚Äì accumulation of organic acids,\n",
      "  ‚Äì or buffer capacity issues in the medium.\n",
      "‚Ä¢ Compare FB1 profiles across vessels and across runs; stable/high producers often show a consistent, moderate base profile during induction.\n",
      "\n",
      "Feeds (FA1, FB1, FC1)\n",
      "----------------------\n",
      "‚Ä¢ Glycerol phase:\n",
      "  ‚Äì High-glycerol batch to OD600 ~50‚Äì100.\n",
      "  ‚Äì Keep DO above ~35%; let cells reach high biomass before MeOH.\n",
      "‚Ä¢ MeOH adaptation:\n",
      "  ‚Äì Ramp methanol feed slowly to avoid DO crashes and off-gas spikes.\n",
      "  ‚Äì Aim for residual MeOH ~0.5‚Äì1.0 g/L in adaptation.\n",
      "‚Ä¢ Induction:\n",
      "  ‚Äì Hold residual MeOH in ~0.5‚Äì2.0 g/L band (measured by HPLC once available).\n",
      "  ‚Äì Consider sorbitol co-feed at 20‚Äì40% of total carbon to reduce MeOH toxicity.\n",
      "‚Ä¢ Foaming:\n",
      "  ‚Äì Plot FC1 vs time and correlate with DO and airflow; large FC1 pulses often co-occur with air/DO disturbances.\n",
      "\n",
      "OD600 and (future) collagen titer\n",
      "---------------------------------\n",
      "‚Ä¢ Once HPLC is online, link OD600 + collagen titer to:\n",
      "  ‚Äì DO profile,\n",
      "  ‚Äì methanol residuals,\n",
      "  ‚Äì temperature profile,\n",
      "  ‚Äì base and antifoam usage.\n",
      "‚Ä¢ Calculate simple indicators:\n",
      "  ‚Äì Space-time yield (STY) = g collagen / (L * h),\n",
      "  ‚Äì qP (specific productivity) if dry weight or cell volume is available.\n",
      "\n",
      "Example DoE directions\n",
      "----------------------\n",
      "‚Ä¢ Factors to explore (screening 3‚Äì4 runs at a time):\n",
      "  ‚Äì DO setpoint: 25%, 30%, 35%.\n",
      "  ‚Äì Temperature: 22, 25, 28 ¬∞C.\n",
      "  ‚Äì MeOH residual: 0.5, 1.0, 2.0 g/L (once HPLC exists).\n",
      "  ‚Äì Sorbitol fraction of carbon: 0, 0.25, 0.5.\n",
      "‚Ä¢ Responses:\n",
      "  ‚Äì Collagen titer (g/L),\n",
      "  ‚Äì STY (g/L/h),\n",
      "  ‚Äì OD600 at harvest,\n",
      "  ‚Äì Residual MeOH and glycerol profiles,\n",
      "  ‚Äì Oxygen uptake (qualitatively from DO/air/rpm trends),\n",
      "  ‚Äì Base and antifoam usage.\n",
      "\n",
      "When HPLC is available\n",
      "----------------------\n",
      "‚Ä¢ Ingest HPLC sheets to get:\n",
      "  ‚Äì Residual glycerol time series,\n",
      "  ‚Äì Residual methanol time series,\n",
      "  ‚Äì Eventually collagen peak area / concentration.\n",
      "‚Ä¢ Integrate these into the same master table and plots:\n",
      "  ‚Äì Residual glycerol vs time (to confirm derepression and depletion).\n",
      "  ‚Äì Residual MeOH vs time (to see control quality).\n",
      "  ‚Äì Collagen vs time to compute onset and slope of expression.\n",
      "\n",
      "References (selected)\n",
      "---------------------\n",
      "(You can paste these into reports or slide decks.)\n",
      "\n",
      "Pichia fermentation & methanol systems\n",
      "1. Cereghino, J. L., & Cregg, J. M. (2000). Heterologous protein expression in the methylotrophic yeast Pichia pastoris. FEMS Microbiology Reviews, 24(1), 45‚Äì66.\n",
      "2. Potvin, G., Ahmad, A., & Zhang, Z. (2012). Bioprocess engineering aspects of heterologous protein production in Pichia pastoris. Biotechnology Advances, 30(6), 1219‚Äì1230.\n",
      "3. Jungo, C., et al. (2007). Mixed-feed strategies for high-level recombinant protein production with Pichia pastoris. Biotechnology and Bioengineering.\n",
      "\n",
      "Feed strategies (glycerol, methanol, sorbitol)\n",
      "4. Jahic, M., et al. (2002). Optimizing methanol feed rate for recombinant protein production in Pichia pastoris. Journal of Biotechnology, 98(3), 269‚Äì283.\n",
      "5. Cos, O., et al. (2006). Sorbitol co-feeding reduces methanol toxicity in Pichia pastoris. Microbial Cell Factories.\n",
      "\n",
      "DO, oxygen transfer, agitation, airflow\n",
      "6. Looser, V., et al. (2015). Pichia pastoris fed-batch strategies: DO control and specific oxygen uptake rate. Biotechnology Progress, 31(5), 1255‚Äì1266.\n",
      "7. Eppendorf Application Note 339. Strategies for high-density fermentation with Pichia pastoris in DASGIP parallel bioreactor systems.\n",
      "\n",
      "pH optimization\n",
      "8. √áelik, E., & √áalƒ±k, P. (2012). pH optimization for recombinant protein expression in Pichia pastoris. Biochemical Engineering Journal.\n",
      "\n",
      "Collagen & structural proteins\n",
      "9. Xu, L., et al. (2020). Production of recombinant human collagen in Pichia pastoris. Frontiers in Bioengineering and Biotechnology.\n",
      "10. Hsu, C., et al. (2019). Engineered yeast for collagen-like protein production. ACS Synthetic Biology.\n",
      "\n",
      "Hybrid / model-based optimization\n",
      "11. AspenTech White Paper. Hybrid Models for Fermentation Optimization.\n",
      "12. Basetwo AI Technical Brief. Real-time DO/methanol predictive modeling in bioprocessing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Cell 8 ‚Äì Suggestions & References\n",
    "# ================================\n",
    "\n",
    "def print_suggestions_and_references():\n",
    "    text = r\"\"\"\n",
    "=== Suggestions for Pichia pastoris Collagen Fermentations (Liven Proteins) ===\n",
    "\n",
    "Upstream strategy (current template)\n",
    "------------------------------------\n",
    "‚Ä¢ Use glycerol batch ‚Üí derepression ‚Üí MeOH adaptation ‚Üí MeOH induction.\n",
    "‚Ä¢ FA1: glycerol before induction, methanol after (see Batch record 'Methanol fed started at ‚Ä¶ h').\n",
    "‚Ä¢ FB1: base feed for pH control.\n",
    "‚Ä¢ FC1: antifoam; monitor spikes as a proxy for foaming events.\n",
    "\n",
    "DO control\n",
    "----------\n",
    "‚Ä¢ Target DO setpoint 25‚Äì35% during MeOH induction.\n",
    "‚Ä¢ Avoid sustained DO < 15% for more than ~10 minutes (oxygen limitation, stress responses).\n",
    "‚Ä¢ Use airflow increase first, then agitation (rpm) in your DO cascade.\n",
    "‚Ä¢ Track specific oxygen uptake rate (sOUR) where possible as an internal 'demand' signal.\n",
    "\n",
    "Temperature\n",
    "-----------\n",
    "‚Ä¢ For collagen-like proteins, slightly lower temps often help folding and secretion.\n",
    "‚Ä¢ Typical induction window: 22‚Äì28 ¬∞C; a good starting point is 25‚Äì26 ¬∞C.\n",
    "‚Ä¢ Run a small DoE around 22, 25, 28 ¬∞C at otherwise matched conditions.\n",
    "\n",
    "pH & base usage\n",
    "---------------\n",
    "‚Ä¢ Maintain pH between 5.2 and 5.8 (start at 5.5).\n",
    "‚Ä¢ High base consumption + drifting pH may indicate:\n",
    "  ‚Äì overload of methanol,\n",
    "  ‚Äì accumulation of organic acids,\n",
    "  ‚Äì or buffer capacity issues in the medium.\n",
    "‚Ä¢ Compare FB1 profiles across vessels and across runs; stable/high producers often show a consistent, moderate base profile during induction.\n",
    "\n",
    "Feeds (FA1, FB1, FC1)\n",
    "----------------------\n",
    "‚Ä¢ Glycerol phase:\n",
    "  ‚Äì High-glycerol batch to OD600 ~50‚Äì100.\n",
    "  ‚Äì Keep DO above ~35%; let cells reach high biomass before MeOH.\n",
    "‚Ä¢ MeOH adaptation:\n",
    "  ‚Äì Ramp methanol feed slowly to avoid DO crashes and off-gas spikes.\n",
    "  ‚Äì Aim for residual MeOH ~0.5‚Äì1.0 g/L in adaptation.\n",
    "‚Ä¢ Induction:\n",
    "  ‚Äì Hold residual MeOH in ~0.5‚Äì2.0 g/L band (measured by HPLC once available).\n",
    "  ‚Äì Consider sorbitol co-feed at 20‚Äì40% of total carbon to reduce MeOH toxicity.\n",
    "‚Ä¢ Foaming:\n",
    "  ‚Äì Plot FC1 vs time and correlate with DO and airflow; large FC1 pulses often co-occur with air/DO disturbances.\n",
    "\n",
    "OD600 and (future) collagen titer\n",
    "---------------------------------\n",
    "‚Ä¢ Once HPLC is online, link OD600 + collagen titer to:\n",
    "  ‚Äì DO profile,\n",
    "  ‚Äì methanol residuals,\n",
    "  ‚Äì temperature profile,\n",
    "  ‚Äì base and antifoam usage.\n",
    "‚Ä¢ Calculate simple indicators:\n",
    "  ‚Äì Space-time yield (STY) = g collagen / (L * h),\n",
    "  ‚Äì qP (specific productivity) if dry weight or cell volume is available.\n",
    "\n",
    "Example DoE directions\n",
    "----------------------\n",
    "‚Ä¢ Factors to explore (screening 3‚Äì4 runs at a time):\n",
    "  ‚Äì DO setpoint: 25%, 30%, 35%.\n",
    "  ‚Äì Temperature: 22, 25, 28 ¬∞C.\n",
    "  ‚Äì MeOH residual: 0.5, 1.0, 2.0 g/L (once HPLC exists).\n",
    "  ‚Äì Sorbitol fraction of carbon: 0, 0.25, 0.5.\n",
    "‚Ä¢ Responses:\n",
    "  ‚Äì Collagen titer (g/L),\n",
    "  ‚Äì STY (g/L/h),\n",
    "  ‚Äì OD600 at harvest,\n",
    "  ‚Äì Residual MeOH and glycerol profiles,\n",
    "  ‚Äì Oxygen uptake (qualitatively from DO/air/rpm trends),\n",
    "  ‚Äì Base and antifoam usage.\n",
    "\n",
    "When HPLC is available\n",
    "----------------------\n",
    "‚Ä¢ Ingest HPLC sheets to get:\n",
    "  ‚Äì Residual glycerol time series,\n",
    "  ‚Äì Residual methanol time series,\n",
    "  ‚Äì Eventually collagen peak area / concentration.\n",
    "‚Ä¢ Integrate these into the same master table and plots:\n",
    "  ‚Äì Residual glycerol vs time (to confirm derepression and depletion).\n",
    "  ‚Äì Residual MeOH vs time (to see control quality).\n",
    "  ‚Äì Collagen vs time to compute onset and slope of expression.\n",
    "\n",
    "References (selected)\n",
    "---------------------\n",
    "(You can paste these into reports or slide decks.)\n",
    "\n",
    "Pichia fermentation & methanol systems\n",
    "1. Cereghino, J. L., & Cregg, J. M. (2000). Heterologous protein expression in the methylotrophic yeast Pichia pastoris. FEMS Microbiology Reviews, 24(1), 45‚Äì66.\n",
    "2. Potvin, G., Ahmad, A., & Zhang, Z. (2012). Bioprocess engineering aspects of heterologous protein production in Pichia pastoris. Biotechnology Advances, 30(6), 1219‚Äì1230.\n",
    "3. Jungo, C., et al. (2007). Mixed-feed strategies for high-level recombinant protein production with Pichia pastoris. Biotechnology and Bioengineering.\n",
    "\n",
    "Feed strategies (glycerol, methanol, sorbitol)\n",
    "4. Jahic, M., et al. (2002). Optimizing methanol feed rate for recombinant protein production in Pichia pastoris. Journal of Biotechnology, 98(3), 269‚Äì283.\n",
    "5. Cos, O., et al. (2006). Sorbitol co-feeding reduces methanol toxicity in Pichia pastoris. Microbial Cell Factories.\n",
    "\n",
    "DO, oxygen transfer, agitation, airflow\n",
    "6. Looser, V., et al. (2015). Pichia pastoris fed-batch strategies: DO control and specific oxygen uptake rate. Biotechnology Progress, 31(5), 1255‚Äì1266.\n",
    "7. Eppendorf Application Note 339. Strategies for high-density fermentation with Pichia pastoris in DASGIP parallel bioreactor systems.\n",
    "\n",
    "pH optimization\n",
    "8. √áelik, E., & √áalƒ±k, P. (2012). pH optimization for recombinant protein expression in Pichia pastoris. Biochemical Engineering Journal.\n",
    "\n",
    "Collagen & structural proteins\n",
    "9. Xu, L., et al. (2020). Production of recombinant human collagen in Pichia pastoris. Frontiers in Bioengineering and Biotechnology.\n",
    "10. Hsu, C., et al. (2019). Engineered yeast for collagen-like protein production. ACS Synthetic Biology.\n",
    "\n",
    "Hybrid / model-based optimization\n",
    "11. AspenTech White Paper. Hybrid Models for Fermentation Optimization.\n",
    "12. Basetwo AI Technical Brief. Real-time DO/methanol predictive modeling in bioprocessing.\n",
    "\"\"\"\n",
    "    print(text)\n",
    "\n",
    "\n",
    "print_suggestions_and_references()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08d319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_556e0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_556e0_level0_col0\" class=\"col_heading level0 col0\" >Run_ID</th>\n",
       "      <th id=\"T_556e0_level0_col1\" class=\"col_heading level0 col1\" >DO_setpoint_%</th>\n",
       "      <th id=\"T_556e0_level0_col2\" class=\"col_heading level0 col2\" >Temperature_C</th>\n",
       "      <th id=\"T_556e0_level0_col3\" class=\"col_heading level0 col3\" >MeOH_residual_gL</th>\n",
       "      <th id=\"T_556e0_level0_col4\" class=\"col_heading level0 col4\" >Sorbitol_frac_carbon</th>\n",
       "      <th id=\"T_556e0_level0_col5\" class=\"col_heading level0 col5\" >pH_setpoint</th>\n",
       "      <th id=\"T_556e0_level0_col6\" class=\"col_heading level0 col6\" >Agitation_Air_strategy</th>\n",
       "      <th id=\"T_556e0_level0_col7\" class=\"col_heading level0 col7\" >Induction_duration_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_556e0_row0_col0\" class=\"data row0 col0\" >Run_1</td>\n",
       "      <td id=\"T_556e0_row0_col1\" class=\"data row0 col1\" >25</td>\n",
       "      <td id=\"T_556e0_row0_col2\" class=\"data row0 col2\" >22</td>\n",
       "      <td id=\"T_556e0_row0_col3\" class=\"data row0 col3\" >0.500000</td>\n",
       "      <td id=\"T_556e0_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "      <td id=\"T_556e0_row0_col5\" class=\"data row0 col5\" >5.500000</td>\n",
       "      <td id=\"T_556e0_row0_col6\" class=\"data row0 col6\" >Maintain DO via airflow first, then rpm</td>\n",
       "      <td id=\"T_556e0_row0_col7\" class=\"data row0 col7\" >18‚Äì48 (hold residual MeOH steady)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_556e0_row1_col0\" class=\"data row1 col0\" >Run_2</td>\n",
       "      <td id=\"T_556e0_row1_col1\" class=\"data row1 col1\" >35</td>\n",
       "      <td id=\"T_556e0_row1_col2\" class=\"data row1 col2\" >28</td>\n",
       "      <td id=\"T_556e0_row1_col3\" class=\"data row1 col3\" >2.000000</td>\n",
       "      <td id=\"T_556e0_row1_col4\" class=\"data row1 col4\" >0.500000</td>\n",
       "      <td id=\"T_556e0_row1_col5\" class=\"data row1 col5\" >5.500000</td>\n",
       "      <td id=\"T_556e0_row1_col6\" class=\"data row1 col6\" >Maintain DO via airflow first, then rpm</td>\n",
       "      <td id=\"T_556e0_row1_col7\" class=\"data row1 col7\" >18‚Äì48 (hold residual MeOH steady)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_556e0_row2_col0\" class=\"data row2 col0\" >Run_3</td>\n",
       "      <td id=\"T_556e0_row2_col1\" class=\"data row2 col1\" >25</td>\n",
       "      <td id=\"T_556e0_row2_col2\" class=\"data row2 col2\" >22</td>\n",
       "      <td id=\"T_556e0_row2_col3\" class=\"data row2 col3\" >2.000000</td>\n",
       "      <td id=\"T_556e0_row2_col4\" class=\"data row2 col4\" >0.500000</td>\n",
       "      <td id=\"T_556e0_row2_col5\" class=\"data row2 col5\" >5.500000</td>\n",
       "      <td id=\"T_556e0_row2_col6\" class=\"data row2 col6\" >Maintain DO via airflow first, then rpm</td>\n",
       "      <td id=\"T_556e0_row2_col7\" class=\"data row2 col7\" >18‚Äì48 (hold residual MeOH steady)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_556e0_row3_col0\" class=\"data row3 col0\" >Run_4</td>\n",
       "      <td id=\"T_556e0_row3_col1\" class=\"data row3 col1\" >25</td>\n",
       "      <td id=\"T_556e0_row3_col2\" class=\"data row3 col2\" >28</td>\n",
       "      <td id=\"T_556e0_row3_col3\" class=\"data row3 col3\" >0.500000</td>\n",
       "      <td id=\"T_556e0_row3_col4\" class=\"data row3 col4\" >0.500000</td>\n",
       "      <td id=\"T_556e0_row3_col5\" class=\"data row3 col5\" >5.500000</td>\n",
       "      <td id=\"T_556e0_row3_col6\" class=\"data row3 col6\" >Maintain DO via airflow first, then rpm</td>\n",
       "      <td id=\"T_556e0_row3_col7\" class=\"data row3 col7\" >18‚Äì48 (hold residual MeOH steady)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1143b7c20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ DOE (4 runs) saved to: /Users/peyma/Desktop/PROJECT 24-0048 (CII Prototype Strain Bioreactor Validation)/outputs/DOE_next4_runs.csv\n",
      "\n",
      "- Run_1: DO 25% | T 22¬∞C | MeOH residual 0.5 g/L | Sorbitol 0 | pH 5.5 ‚Äî Maintain DO via airflow first, then rpm\n",
      "- Run_2: DO 35% | T 28¬∞C | MeOH residual 2 g/L | Sorbitol 0.5 | pH 5.5 ‚Äî Maintain DO via airflow first, then rpm\n",
      "- Run_3: DO 25% | T 22¬∞C | MeOH residual 2 g/L | Sorbitol 0.5 | pH 5.5 ‚Äî Maintain DO via airflow first, then rpm\n",
      "- Run_4: DO 25% | T 28¬∞C | MeOH residual 0.5 g/L | Sorbitol 0.5 | pH 5.5 ‚Äî Maintain DO via airflow first, then rpm\n",
      "\n",
      "Responses to record: Collagen titer, STY, OD600 at harvest, qP, residual MeOH profile, O‚ÇÇ uptake, base addition.\n"
     ]
    }
   ],
   "source": [
    "# === DOE: Next 4 Runs (space-filling design) ===\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = Path.cwd() / \"outputs\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# --- Factor levels (edit as needed) ---\n",
    "levels = {\n",
    "    \"DO_setpoint_%\": [25, 30, 35],\n",
    "    \"Temperature_C\": [22, 25, 28],\n",
    "    \"MeOH_residual_gL\": [0.5, 1.0, 2.0],\n",
    "    \"Sorbitol_frac_carbon\": [0.0, 0.25, 0.5],\n",
    "}\n",
    "\n",
    "constants = {\n",
    "    \"pH_setpoint\": 5.5,\n",
    "    \"Agitation_Air_strategy\": \"Maintain DO via airflow first, then rpm\",\n",
    "    \"Induction_duration_h\": \"18‚Äì48 (hold residual MeOH steady)\",\n",
    "}\n",
    "\n",
    "# --- Candidate grid ---\n",
    "cand = pd.DataFrame(list(itertools.product(*levels.values())), columns=list(levels.keys()))\n",
    "\n",
    "# Optional rule: skip extreme corners\n",
    "mask = ~((cand[\"DO_setpoint_%\"] == 25) & (cand[\"MeOH_residual_gL\"] == 2.0) & (cand[\"Sorbitol_frac_carbon\"] == 0.0))\n",
    "cand = cand[mask].reset_index(drop=True)\n",
    "\n",
    "# --- Scale for space-filling distance ---\n",
    "scaled = cand.copy()\n",
    "for col in levels:\n",
    "    lo, hi = min(levels[col]), max(levels[col])\n",
    "    scaled[col] = (scaled[col] - lo) / (hi - lo)\n",
    "X = scaled.values\n",
    "\n",
    "# --- Space-filling pick for 4 runs ---\n",
    "n_runs = 4\n",
    "sel = []\n",
    "# first: farthest from center\n",
    "center = np.full(X.shape[1], 0.5)\n",
    "d_center = np.linalg.norm(X - center, axis=1)\n",
    "sel.append(int(np.argmax(d_center)))\n",
    "\n",
    "# next: iteratively pick farthest from all selected\n",
    "def min_dist_to_selected(X, sel_idx):\n",
    "    sel_points = X[sel_idx]\n",
    "    d = np.stack([np.linalg.norm(X - s, axis=1) for s in sel_points], axis=1)\n",
    "    return d.min(axis=1)\n",
    "\n",
    "while len(sel) < n_runs:\n",
    "    dmin = min_dist_to_selected(X, sel)\n",
    "    dmin[sel] = -1\n",
    "    sel.append(int(np.argmax(dmin)))\n",
    "\n",
    "doe = cand.iloc[sel].copy().reset_index(drop=True)\n",
    "\n",
    "# --- Add constants ---\n",
    "for k, v in constants.items():\n",
    "    doe[k] = v\n",
    "doe.insert(0, \"Run_ID\", [f\"Run_{i+1}\" for i in range(len(doe))])\n",
    "\n",
    "col_order = [\"Run_ID\"] + list(levels.keys()) + list(constants.keys())\n",
    "doe = doe[col_order]\n",
    "\n",
    "# --- Save & display ---\n",
    "path = output_dir / \"DOE_next4_runs.csv\"\n",
    "doe.to_csv(path, index=False)\n",
    "\n",
    "# Use safe display if jinja2 not installed\n",
    "try:\n",
    "    display(doe.style.hide(axis='index'))\n",
    "except Exception:\n",
    "    print(doe)\n",
    "\n",
    "print(f\"\\n‚úÖ DOE (4 runs) saved to: {path}\\n\")\n",
    "\n",
    "# --- Operational notes ---\n",
    "for _, r in doe.iterrows():\n",
    "    print(\n",
    "        f\"- {r['Run_ID']}: DO {r['DO_setpoint_%']:.0f}% | T {r['Temperature_C']:.0f}¬∞C | \"\n",
    "        f\"MeOH residual {r['MeOH_residual_gL']:.2g} g/L | Sorbitol {r['Sorbitol_frac_carbon']:.2g} \"\n",
    "        f\"| pH {r['pH_setpoint']} ‚Äî {r['Agitation_Air_strategy']}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nResponses to record: Collagen titer, STY, OD600 at harvest, qP, residual MeOH profile, O‚ÇÇ uptake, base addition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ed9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build UI with available experiments\n",
    "build_experiment_buttons(master)\n",
    "\n",
    "# Show dashboard\n",
    "display(dashboard_title, exp_select_box, param_buttons_box, plot_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
